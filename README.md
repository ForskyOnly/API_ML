Voici une version mise Ã  jour et intÃ©grÃ©e du README, incorporant les informations sur le ML Ops et le dashboard :

```markdown
# ğŸš€ API de Classification des DonnÃ©es Ã  Risque

## ğŸ“‹ Table des matiÃ¨res
- [Ã€ propos du projet](#Ã -propos-du-projet)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [ML Ops et Dashboard](#ml-ops-et-dashboard)
- [DÃ©ploiement](#dÃ©ploiement)
- [Contribution](#contribution)
- [Licence](#licence)

## ğŸ¯ Ã€ propos du projet

Ce projet est une API de classification des donnÃ©es Ã  risque, utilisant des modÃ¨les de machine learning pour prÃ©dire le niveau de risque associÃ© Ã  un texte donnÃ©.

## âœ¨ FonctionnalitÃ©s

- ğŸ” Classification de texte en temps rÃ©el
- ğŸ” Authentification par clÃ© API
- ğŸ“Š Utilisation de modÃ¨les MLflow pour les prÃ©dictions
- ğŸ³ Conteneurisation avec Docker pour un dÃ©ploiement facile
- ğŸ“ˆ Dashboard interactif pour le suivi des performances des modÃ¨les

## ğŸ›  PrÃ©requis

- Python 3.9+
- Docker
- Compte Azure (pour le dÃ©ploiement)

## ğŸš€ Installation

1. Clonez le dÃ©pÃ´t :
   ```
   git clone https://github.com/ForskyOnly/API_ML/.git
   cd api_ml
   ```

2. CrÃ©ez un environnement virtuel :
   ```
   python -m venv venv
   source venv/bin/activate  # Sur Windows, utilisez `venv\Scripts\activate`
   ```

3. Installez les dÃ©pendances :
   ```
   pip install -r requirements.txt
   ```

4. Configurez les variables d'environnement :
   ```
   cp .env.example .env
   ```
   Ã‰ditez le fichier `.env` avec vos propres valeurs.

## ğŸ–¥ Utilisation

Pour lancer l'API localement :

```
uvicorn api.app.main:app --reload
```

L'API sera accessible Ã  l'adresse `http://localhost:8000`.

## ğŸ›  ML Ops et Dashboard

### MLflow

MLflow est utilisÃ© pour le suivi des expÃ©riences, la gestion des modÃ¨les et le dÃ©ploiement.

Pour accÃ©der Ã  l'interface MLflow, exÃ©cutez :

```
mlflow ui
```

### Dashboard de Monitoring

Un dashboard interactif a Ã©tÃ© dÃ©veloppÃ© avec Streamlit pour visualiser les performances des modÃ¨les.

Pour lancer le dashboard :

```
streamlit run ml_ops/dashboard.py
```

#### FonctionnalitÃ©s principales du dashboard :

1. SÃ©lection du modÃ¨le
2. Affichage des meilleurs paramÃ¨tres
3. Visualisation des mÃ©triques (prÃ©cision, rappel, score F1)
4. Analyse du drift
5. Matrice de confusion
6. Comparaison des modÃ¨les
7. Historique des runs

### IntÃ©gration Continue / DÃ©ploiement Continu (CI/CD)

Nous utilisons GitHub Actions pour automatiser les tests, la construction des images Docker et le dÃ©ploiement sur Azure.

### Monitoring en Production

- Logs : Utilisation du logging Python standard, capturÃ© et gÃ©rÃ© par Azure.
- Alertes : ConfigurÃ©es dans Azure pour notifier en cas de problÃ¨mes.

## ğŸŒ DÃ©ploiement

1. Construisez l'image Docker :
   ```
   docker build -t votre-image:tag .
   ```

2. Poussez l'image vers Azure Container Registry :
   ```
   docker push votre-acr.azurecr.io/votre-image:tag
   ```

3. DÃ©ployez sur Azure Container Instances en utilisant le portail Azure ou Azure CLI.

4. Configurez les variables d'environnement dans Azure :
   - API_KEY
   - API_KEY_NAME
   - MLFLOW_TRACKING_URI
   - BEST_RUN_ID

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Consultez le fichier `CONTRIBUTING.md` pour plus d'informations.

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.
```

